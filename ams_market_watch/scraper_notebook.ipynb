{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-03T14:40:21.132648Z",
     "start_time": "2020-05-03T14:40:21.123694Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:95% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:95% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-03T14:40:22.995005Z",
     "start_time": "2020-05-03T14:40:21.576419Z"
    }
   },
   "outputs": [],
   "source": [
    "#Essentials\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import re\n",
    "import datetime as dt\n",
    "import time\n",
    "\n",
    "# For scraping\n",
    "import requests\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver import ChromeOptions\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.support.ui import Select\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support.wait import WebDriverWait\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "\n",
    "#Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "from matplotlib.pylab import rcParams\n",
    "rcParams['figure.figsize'] = 25, 6\n",
    "rcParams.update({'font.size': 14})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gather data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-03T14:40:23.039632Z",
     "start_time": "2020-05-03T14:40:22.997235Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_webdriver():\n",
    "    \"\"\"Simply starts a new Chrome browser instance\"\"\"\n",
    "    #Load .env file contents\n",
    "    load_dotenv()\n",
    "    \n",
    "    chromedriver = os.environ.get('webdriver_loc')           #  path to the chromedriver executable\n",
    "    os.environ[\"webdriver.chrome.driver\"] = chromedriver\n",
    "#   Define optional settings and go to the website\n",
    "    chrome_options = ChromeOptions()\n",
    "    \n",
    "#     options.add_argument(\"--headless\")\n",
    "#     options.add_argument(\"--disable-notifications\")\n",
    "#     driver = webdriver.Chrome(executable_path=chromedriver, chrome_options=chrome_options, options=options)\n",
    "    \n",
    "    driver = webdriver.Chrome(executable_path=chromedriver, chrome_options=chrome_options)\n",
    "    return driver\n",
    "    \n",
    "\n",
    "def login_google(driver):\n",
    "    \"\"\"In order to avoid reCAPTCHA's I need to first log in to my google account.\n",
    "    This function does just that\"\"\"\n",
    "    #Load .env file contents\n",
    "    load_dotenv()\n",
    "    google_url = os.environ.get('google_url')\n",
    "    \n",
    "    driver.get(google_url)\n",
    "    time.sleep(1)\n",
    "    \n",
    "    WebDriverWait(driver, 5).until(EC.visibility_of_element_located((By.ID, \"identifierId\")))\n",
    "    \n",
    "    username_field = driver.find_element_by_id('identifierId')\n",
    "    username_field.send_keys(os.environ.get('username'))\n",
    "    username_field.send_keys(Keys.RETURN)\n",
    "    time.sleep(1)\n",
    "    password_field = driver.find_element_by_name('password')\n",
    "    password_field.send_keys(os.environ.get('secret_g'))\n",
    "    password_field.send_keys(Keys.RETURN)\n",
    "\n",
    "\n",
    "def solve_recaptcha(driver):\n",
    "    \"\"\"Click reCAPTCHA checkbox. If the test with images comes up, a person has to solve the 'puzzle'.\n",
    "    The solve_recaptha function waits 30 seconds for the 'puzzle' to be solved\"\"\"\n",
    "    time.sleep(5)\n",
    "\n",
    "    try:\n",
    "        frame = driver.find_element_by_xpath('//iframe[contains(@src, \"recaptcha\")]')\n",
    "        driver.switch_to.frame(frame)\n",
    "        driver.find_element_by_xpath(\"//*[@id='recaptcha-anchor']\").click()\n",
    "        driver.switch_to.default_content()\n",
    "        try:            \n",
    "            # Try accessing recaptcha ancor again to verify if the page has changed to the cookies settings page yet or not\n",
    "            driver.switch_to.frame(frame)\n",
    "            tickbox = driver.find_element_by_xpath(\"//*[@id='recaptcha-anchor']\")\n",
    "            print('Human intervention needed. Please solve reCAPTCHA puzzle.')\n",
    "            \n",
    "            # Then go through the process of clicking the tickbox again, but this time wait for a \"human\" to solve the puzzle\n",
    "            tickbox.click()\n",
    "            time.sleep(30)\n",
    "            \n",
    "            # Check if the screen changed to the cookies settings\n",
    "#             try:\n",
    "#                 driver.find_element_by_xpath(\"//*[@id='AllowPersonalisatie']\").text\n",
    "#                 print('reCAPTHA solved successfully!')\n",
    "#             else:\n",
    "#                 print('Could not solve reCAPTHA :(')\n",
    "        except:\n",
    "            print('reCAPTHA solved successfully!')\n",
    "    except:\n",
    "        print('Aaoooohh, got cought by distil software...or did not solve reCAPTHA in time.')\n",
    "    \n",
    "\n",
    "# Not currently used!\n",
    "def set_cookies(driver):\n",
    "    \"\"\"This function sets the minimal cookie settings and takes us to the filters page\"\"\"\n",
    "    # Load .env file contents\n",
    "    load_dotenv()\n",
    "    funda_url = os.environ.get('funda_url')\n",
    "    cookie_url = funda_url + '/cookiebeleid/?ReturnUrl=%2f'\n",
    "    driver.get(cookie_url)\n",
    "\n",
    "    element = WebDriverWait(driver, 5).until(EC.visibility_of_element_located((By.XPATH, \"//a[contains(text(), 'Aangepast')]\")))\n",
    "    element.click()\n",
    "\n",
    "    cookie_box = WebDriverWait(driver, 5).until(EC.visibility_of_element_located((By.XPATH, \"//div[@id='cookie-preference-aangepast']\")))\n",
    "    save_cookies = cookie_box.find_element_by_xpath(\".//button[contains(text(), 'Cookievoorkeuren opslaan')]\")\n",
    "    save_cookies.click()\n",
    "    \n",
    "def accept_all_cookies(driver):\n",
    "    \"\"\"Simply clicks the button to accept all pre-set cookies\"\"\"\n",
    "    save_cookies = driver.find_element_by_xpath(\".//button[contains(text(), 'Accepteer alle cookies')]\")\n",
    "    save_cookies.click()\n",
    "    \n",
    "      \n",
    "def log_in(driver):\n",
    "    \"\"\"Uses the credentials defined in the .env file to log in and saves properties matching desired criteria to the favourites\"\"\"\n",
    "    # This is to load environment variables\n",
    "    load_dotenv()\n",
    "    login_url = os.environ.get('login_url')\n",
    "    url = os.environ.get('funda_url')\n",
    "    \n",
    "    driver.get(login_url)\n",
    "    time.sleep(3)\n",
    "    accept_all_cookies(driver)\n",
    "    \n",
    "    # Then proceed to log in\n",
    "    WebDriverWait(driver, 5).until(EC.visibility_of_element_located((By.ID, \"Username\")))\n",
    "    \n",
    "    username_field = driver.find_element_by_id('Username')\n",
    "    username_field.send_keys(os.environ.get('username'))\n",
    "    password_field = driver.find_element_by_id('Password')\n",
    "    password_field.send_keys(os.environ.get('secret_f'))\n",
    "    time.sleep(3)\n",
    "    driver.find_element_by_xpath(\"//button[contains(text(), 'Log in')]\").click()\n",
    "    driver.get(url)\n",
    "    \n",
    "    \n",
    "def load_funda(driver):\n",
    "    \"\"\"This function simply starts the chrome driver (from a defined location on the machine).\"\"\"\n",
    "    # Load .env file contents\n",
    "    load_dotenv()\n",
    "    funda_url = os.environ.get('funda_url')\n",
    "    \n",
    "    log_in(driver)\n",
    "    time.sleep(3)\n",
    "    driver.get(funda_url)\n",
    "    time.sleep(3)\n",
    "\n",
    "    try:\n",
    "        accept_all_cookies(driver)\n",
    "        # Makes sure that \"For Sale\" category is selected\n",
    "        current_selection = driver.find_element_by_class_name('is-active').text\n",
    "        if current_selection == 'For Sale':\n",
    "            print('All set for filtering!')\n",
    "        else:\n",
    "            driver.get(funda_url+'koop/')\n",
    "            print('All set for filtering!')\n",
    "    except:\n",
    "        try:\n",
    "            solve_recaptcha(driver)\n",
    "            accept_all_cookies(driver)\n",
    "            time.sleep(5)\n",
    "            current_selection = driver.find_element_by_class_name('is-active').text\n",
    "            \n",
    "            # Makes sure that \"For Sale\" category is selected\n",
    "            if current_selection == 'For Sale':\n",
    "                print('All set for filtering!')\n",
    "            else:\n",
    "                driver.get(funda_url+'koop/')\n",
    "                print('All set for filtering!')\n",
    "            \n",
    "        except:\n",
    "            print('Something went wrong :(')\n",
    "    \n",
    "    \n",
    "def log_out(driver):\n",
    "    \"\"\"Logs out once the actions are done\"\"\"\n",
    "    # This is to load environment variables\n",
    "    load_dotenv()\n",
    "    \n",
    "    logout_url = os.environ.get('logout_url')\n",
    "    driver.get(logout_url)\n",
    "    \n",
    "    \n",
    "def apply_basic_filters(driver, filter_dict={'location': 'Amsterdam', 'radius': '0', 'min_price': '0', 'max_price': 'ignore_filter'}):\n",
    "    \"\"\"Takes a dictionary of basic filters, applies them and searches for the properties\"\"\"\n",
    "    # Now apply desired filters (e.g. set location to Amsterdam)\n",
    "    WebDriverWait(driver, 5).until(EC.visibility_of_element_located((By.NAME, \"filter_location\")))\n",
    "    filter_loc = driver.find_element_by_name('filter_location')\n",
    "    filter_loc.send_keys(filter_dict['location'])\n",
    "\n",
    "    # Wait for the first dropdown option to appear and select the first option\n",
    "    WebDriverWait(driver, 5).until(EC.visibility_of_element_located((By.CSS_SELECTOR, \".autocomplete-list\")))\n",
    "    filter_loc.send_keys(Keys.ARROW_DOWN)\n",
    "    filter_loc.send_keys(Keys.ENTER)\n",
    "\n",
    "    filter_radius = Select(driver.find_element_by_id('Straal'))\n",
    "    filter_radius.select_by_value(filter_dict['radius'])\n",
    "\n",
    "    filter_price_from = Select(driver.find_element_by_name('filter_KoopprijsVan'))\n",
    "    filter_price_from.select_by_value(filter_dict['min_price'])\n",
    "\n",
    "    filter_price_upto = Select(driver.find_element_by_name('filter_KoopprijsTot'))\n",
    "    filter_price_upto.select_by_value(filter_dict['max_price'])\n",
    "\n",
    "    WebDriverWait(driver, 5).until(EC.visibility_of_element_located((By.XPATH, \"//button[@class='button-primary-alternative']\")))\n",
    "    try:\n",
    "        find_search = driver.find_element_by_xpath(\"//button[contains(text(), 'Search')]\")\n",
    "        find_search.click()\n",
    "    except:\n",
    "        find_zoek = driver.find_element_by_xpath(\"//button[contains(text(), 'Zoek')]\")\n",
    "        find_zoek.click()\n",
    "    \n",
    "    \n",
    "def get_id(html):\n",
    "    \"\"\"Given an html this function retrieves the property id from the html link.\n",
    "    Returns the id\"\"\"\n",
    "    return int(re.findall(r'-\\d*-', html)[0].strip('-'))\n",
    "\n",
    "\n",
    "\n",
    "# Get urls for all the pages and put them into a list\n",
    "def get_url_list(driver):\n",
    "    \"\"\"Records the current url and goes through the website, clicking Next as many times as there are pages.\n",
    "    Returns a list of urls to be used in the get_htmls function.\"\"\"\n",
    "    #Creates a list of urls for all pages\n",
    "    url_list = []\n",
    "    \n",
    "    #Reads the url of the page the driver is currently in and adds it into the list\n",
    "    current_page_url = driver.current_url\n",
    "    url_list.append(current_page_url)\n",
    "    \n",
    "    curr_page_num = 1\n",
    "    count_exceptions = 0\n",
    "    while True:\n",
    "        try:\n",
    "            curr_page_num += 1\n",
    "            #Find \"Next\" button and click it\n",
    "            find_next = driver.find_element_by_xpath(\"//a[@rel='next']\")\n",
    "            find_next.click()\n",
    "            current_page_url = driver.current_url\n",
    "            url_list.append(current_page_url)\n",
    "            time.sleep(10)\n",
    "        except:\n",
    "            #Count the exceptions\n",
    "            count_exceptions += 1\n",
    "            curr_page_num -= 1 #since we are coming back to the same page\n",
    "            #If the there haven't been 3 exceptions yet, sleep for a bit and then continue\n",
    "            if count_exceptions < 3:\n",
    "                time.sleep(10)\n",
    "            else:\n",
    "                #If \"Next\" button isn't there anymore or an error occurs, return the list\n",
    "                #driver.close()\n",
    "                print(f'Url list collection - complete. Last page number was {curr_page_num}.')\n",
    "                return url_list    \n",
    "    \n",
    "    print(f'Url list collection - complete. Last page number was {curr_page_num}.')\n",
    "    return url_list   \n",
    "\n",
    "\n",
    "#Get all the html files for each property ad and put it into a list\n",
    "def get_htmls(driver):\n",
    "    \"\"\"Takes current html list.\n",
    "    Returns updated html list with all the property ad htmls available on the page.\"\"\"    \n",
    "    #Find all property ad htmls\n",
    "    html_list = []\n",
    "    elems = driver.find_elements_by_xpath(\"//a[@href]\")\n",
    "    for elem in elems:\n",
    "        html = elem.get_attribute(\"href\")\n",
    "        if (bool(re.search(r'appartement-\\d+', html)) or bool(re.search('huis-\\d+', html))) and html not in html_list: \n",
    "            html_list.append(html)\n",
    "    return html_list\n",
    "\n",
    "\n",
    "def get_feat_dict(driver, html):\n",
    "    \"\"\"Takes a string with all highlighted features and puts them into a dictionary.\n",
    "    Returns the dictionary\"\"\"\n",
    "    \n",
    "    # Open the html\n",
    "    driver.get(html)\n",
    "\n",
    "    # Click to see all the features available (if button doesn't appear, continue)\n",
    "    try:\n",
    "        WebDriverWait(driver, 7).until(EC.visibility_of_element_located((By.CSS_SELECTOR, \".object-kenmerken-open-button\")))\n",
    "        driver.find_element_by_class_name('object-kenmerken-open-button').click()\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    # Retrieve all the other features now\n",
    "    time.sleep(5)\n",
    "    feature_elems = driver.find_elements_by_class_name('object-kenmerken-body')\n",
    "    feature_string = ''\n",
    "    for elem in feature_elems:\n",
    "        if feature_string == '':\n",
    "            feature_string = elem.text\n",
    "        else:\n",
    "            feature_string += '\\n' + str(elem.text)\n",
    "\n",
    "    categories = ['Auction', 'Transfer of ownership', 'Construction', 'Surface areas and volume', 'Areas', 'Layout', 'Energy', 'What does this mean?', 'Cadastral data', 'Exterior space', 'Storage space', 'Parking', 'VVE (Owners Association) checklist', 'Garage', 'Commercial property']\n",
    "    special_categories = ['Cadastral data', 'Commercial property']\n",
    "    to_delete = ['Cadastral map']\n",
    "    features = feature_string.split('\\n')\n",
    "    feat_list = [feat for feat in features if feat not in to_delete]\n",
    "\n",
    "    feat_dict = {}   \n",
    "    current_index = 0\n",
    "    current_category = ''\n",
    "\n",
    "    for feat in feat_list: \n",
    "        feat = feat_list[current_index]\n",
    "    #     print(feat, current_index)\n",
    "        if feat in categories:\n",
    "            feat_dict[feat] = ''\n",
    "            current_category = feat\n",
    "            current_index += 1        \n",
    "        else:\n",
    "            if current_category in special_categories:\n",
    "                feat_dict[current_category] += str(feat) + '; '\n",
    "                if current_index+2 < len(feat_list):\n",
    "                    current_index += 1\n",
    "                else:\n",
    "                    break\n",
    "            elif feat in feat_dict.keys():\n",
    "                feat_dict[feat+'_'+current_category] = feat_list[current_index+1]\n",
    "                if current_index+2 < len(feat_list):\n",
    "                    current_index += 2\n",
    "                else:\n",
    "                    break\n",
    "            else:\n",
    "                feat_dict[feat] = feat_list[current_index+1]\n",
    "                if current_index+2 < len(feat_list):\n",
    "                    current_index += 2\n",
    "                else:\n",
    "                    break\n",
    "    return feat_dict    \n",
    "    \n",
    "\n",
    "def scrape_data(driver, ads_list=[], url_list_name='', url_index=0):\n",
    "    \"\"\"Reads in the data about each property from a given html files.\n",
    "    Returns a list of dictionaries of all scraped ads (one dictionary per add).\"\"\"\n",
    "\n",
    "    today_timestamp = str(dt.date.today().year) + str(dt.date.today().month) + str(dt.date.today().day)\n",
    "    \n",
    "    if ads_list != []:\n",
    "        feat_dict_list = ads_list\n",
    "    else:\n",
    "        feat_dict_list = []\n",
    "        \n",
    "    new_ad_count = 0\n",
    "    \n",
    "    if url_list_name != '':\n",
    "        with open(f'./Cellar/{url_list_name}.pkl', 'rb') as url_pickle:\n",
    "            url_list = pickle.load(url_pickle)\n",
    "    else:\n",
    "        url_list = get_url_list(driver)\n",
    "        page_count = len(url_list)\n",
    "        with open(f'./Cellar/url_list_{today_timestamp}.pkl', 'wb') as url_pickle:\n",
    "            pickle.dump(url_list, url_pickle)\n",
    "\n",
    "    if url_index != 0:\n",
    "        original_url_list = url_list\n",
    "        url_list = url_list[url_index:]\n",
    "    \n",
    "    for url in url_list:\n",
    "        try:\n",
    "            driver.get(url)\n",
    "            html_list = get_htmls(driver)\n",
    "\n",
    "            for html in html_list:\n",
    "                new_ad_count += 1\n",
    "                property_dict = {}\n",
    "\n",
    "                # To open the property ad\n",
    "                driver.get(html)\n",
    "\n",
    "                # To scrape initial data points\n",
    "                property_dict['property_link'] = html\n",
    "                property_dict['property_id'] = get_id(html)\n",
    "                property_dict['title'] = driver.find_element_by_class_name('object-header__title').text\n",
    "                property_dict['address'] = driver.find_element_by_class_name('object-header__subtitle').text\n",
    "                property_dict['price'] = driver.find_element_by_class_name('object-header__price').text\n",
    "                property_dict['neighbourhood'] = driver.find_element_by_class_name('object-buurt__name').text\n",
    "                property_dict['scraped_date'] = dt.date.today()\n",
    "\n",
    "                other_features = get_feat_dict(driver, html)\n",
    "                for key, value in other_features.items():\n",
    "                    if key not in property_dict.keys():\n",
    "                        property_dict[key] = value\n",
    "                if property_dict not in feat_dict_list:\n",
    "                    feat_dict_list.append(property_dict)\n",
    "                last_url = url\n",
    "        except:\n",
    "            total_ad_count = len(feat_dict_list)\n",
    "            if url_index != 0:\n",
    "                print(last_url, f'This URL is number {original_url_list.index(last_url)} in the original url_list.')\n",
    "            else:\n",
    "                print(last_url, f'This URL is number {url_list.index(last_url)} in the supplied url_list.')\n",
    "            print(f'Finished with an error. Number of ads scraped {new_ad_count}, total number of ads in the list is {total_ad_count}.')\n",
    "            return feat_dict_list\n",
    "    total_ad_count = len(feat_dict_list)\n",
    "    print(f'Finished without errors. Number of ads scraped {new_ad_count}, total number of ads in the list is {total_ad_count}.')\n",
    "    return feat_dict_list\n",
    "\n",
    "\n",
    "\n",
    "def get_recent_ads(driver, days='1', ads_list=[], url_list_name='', url_index=0):\n",
    "    \"\"\"Retrieves only property adverts posted passed in number of days from today (one being today).\n",
    "    The only options for days are 1, 3, 5, 10 and 30.\n",
    "    The default is today (1).\"\"\"\n",
    "    apply_ad_filters(driver, {'filter_type': 'days', 'filter_value': days})\n",
    "    return scrape_data(driver, ads_list=ads_list, url_list_name=url_list_name, url_index=url_index)\n",
    "     \n",
    "\n",
    "def apply_ad_filters(driver, filters_dict={'filter_type': 'days', 'filter_value': '10'}):\n",
    "    \"\"\"Applies specific filters like the size of the property, facilities, etc.\n",
    "    Available filter types: days, status\"\"\"\n",
    "    filter_type = filters_dict['filter_type']\n",
    "    filter_value = filters_dict['filter_value']\n",
    "    type_css_dict = {'days': 'PublicatieDatum-'}\n",
    "    \n",
    "    filters_button_css = \".search-content-header-button-filters.button-tertiary\"\n",
    "    days_on_funda_css = f\"label[class='radio-group-item-label label-text'][for='{type_css_dict[filter_type]}{filter_value}']\"\n",
    "    remove_filter_css = \"button[class='filter-reset__button fd-padding-none button-tertiary is-enhanced']\"\n",
    "    \n",
    "    filters_button = driver.find_element_by_css_selector(filters_button_css)\n",
    "    filter_dof = driver.find_element_by_css_selector(days_on_funda_css)\n",
    "    remove_dof_filter = driver.find_element_by_css_selector(remove_filter_css)\n",
    "    close_filters_pane = driver.find_element_by_css_selector('.button-tertiary.close-search-sidebar')    \n",
    "    \n",
    "    # Click on filters button\n",
    "    try:\n",
    "        time.sleep(3)\n",
    "        filters_button.click()\n",
    "        print('Filters button clicked successfully.')\n",
    "    except:\n",
    "        print('No filters button available, trying to filter directly.')\n",
    "        pass\n",
    "        \n",
    "    try:\n",
    "        time.sleep(4)\n",
    "        filter_dof.click()\n",
    "        print('Days on funda set to \"days\" successfully.')\n",
    "    except:\n",
    "        print('No \"Days on funda\" section available, trying to remove existing filter.')\n",
    "        pass\n",
    "        \n",
    "    try:\n",
    "        WebDriverWait(driver, 3).until(EC.visibility_of_element_located((By.CSS_SELECTOR, remove_filter_css)))\n",
    "        remove_dof_filter.click()\n",
    "        print('Original filter removed.')\n",
    "    except:\n",
    "        print('Not able to remove filters.')\n",
    "        pass\n",
    "\n",
    "    try:\n",
    "        WebDriverWait(driver, 3).until(EC.visibility_of_element_located((By.CSS_SELECTOR, days_on_funda_css)))\n",
    "        next_section = driver.find_element_by_xpath(\"//legend[contains(text(), 'Number of rooms')]\")\n",
    "        ActionChains(driver).move_to_element(next_section).perform()\n",
    "        filter_dof.click()\n",
    "        print('New filter applied successfully.')\n",
    "        # This is to close the filters pane\n",
    "        time.sleep(2)\n",
    "        close_filters_pane.click()\n",
    "        print('Filters pane closed successfully.')\n",
    "    except:\n",
    "        print('Could not set the filter.')\n",
    "    \n",
    "    \n",
    "def save_to_favourites(driver, html):\n",
    "    \"\"\"Saves the given html of the property to favourites\"\"\"\n",
    "    # Goes to the provided html (link)\n",
    "    driver.get(html)\n",
    "    \n",
    "    # Find the \"heart\" icon and clicks it once\n",
    "    driver.find_element_by_class_name('user-save-object').click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-03T14:40:23.055327Z",
     "start_time": "2020-05-03T14:40:23.048731Z"
    }
   },
   "outputs": [],
   "source": [
    "def scrape_new_ads(filters={'location': 'Amsterdam', 'radius': '0', 'min_price': '0', 'max_price': 'ignore_filter'}, \n",
    "                   days=1, url_list_name='', url_index=0, use_new_ads_pickle=False):\n",
    "    \"\"\"Combines different functions into one cohesive function to start scraping new adds\"\"\"\n",
    "    # Start the web driver, log into google and open funda \n",
    "    driver = get_webdriver()\n",
    "    print('Driver initiated successfully!')\n",
    "    \n",
    "    login_google(driver)\n",
    "    print('Logged into Google successfully!')\n",
    "    \n",
    "    # Load funda.nl website. Might need to solve reCAPTCHA puzzle manually\n",
    "    try:\n",
    "        load_funda(driver)\n",
    "        if driver.current_url != 'https://www.funda.nl/distil_identify_cookie.html?httpReferrer=%2Fen%2F%2Fcookiebeleid%2F%3FReturnUrl%3D%252f&distil_rA=2':\n",
    "            print('funda.nl website loaded successfully!')\n",
    "        else:\n",
    "            print('Oh no! Got cought by distil software...Again!')\n",
    "    except:\n",
    "        driver.get('https://www.funda.nl/en/mijn/login/?ReturnUrl=%2Fen%2F')\n",
    "        load_funda(driver)\n",
    "        print('Could not fully load funda.nl/en/ website.')\n",
    "    \n",
    "    # Apply filters\n",
    "    time.sleep(30)\n",
    "    apply_basic_filters(driver, filters)\n",
    "    \n",
    "    # Load existing new_adverts.pkl\n",
    "    if use_new_ads_pickle == True:\n",
    "        with open('./Cellar/new_adverts.pkl', 'rb') as new_ads_pickle:\n",
    "            new_adverts_list = pickle.load(new_ads_pickle)\n",
    "            \n",
    "        # Scrape the data and add to the existing new_adverts pickle\n",
    "        updated_data_df = get_recent_ads(driver, days=days, url_list_name=url_list_name, url_index=url_index, ads_list=new_adverts_list)\n",
    "        print('Scraped successfully (added to existing new_adverts.pkl)!')\n",
    "    else:\n",
    "        # Scrape the data and create a new pickle for it\n",
    "        updated_data_df = get_recent_ads(driver, days=days)\n",
    "        print('Scraped successfully (new pickle file created)!')\n",
    "    \n",
    "    #Log in to funda website and apply filters\n",
    "    log_out(driver)\n",
    "    print('Logged out successfully!')\n",
    "    \n",
    "    # Close the browser, we will not need it anymore\n",
    "    driver.close()\n",
    "    print('Driver closed successfully!')\n",
    "    \n",
    "    # Load newest scraped data into a pickle file\n",
    "    with open('./Cellar/new_adverts.pkl', 'wb') as new_ads_pickle:\n",
    "             pickle.dump(updated_data_df, new_ads_pickle)\n",
    "    print('Pickled successfully!')\n",
    "    \n",
    "    return driver, updated_data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-03T14:40:24.122870Z",
     "start_time": "2020-05-03T14:40:24.120269Z"
    }
   },
   "outputs": [],
   "source": [
    "# https://www.funda.nl/distil_identify_cookie.html?httpReferrer=%2Fen%2F%2Fcookiebeleid%2F%3FReturnUrl%3D%252f&distil_rA=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-03T14:40:24.380152Z",
     "start_time": "2020-05-03T14:40:24.377351Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Manual / testing phase\n",
    "# driver = get_webdriver()\n",
    "# login_google(driver)\n",
    "# load_funda(driver)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-03T17:28:14.020319Z",
     "start_time": "2020-05-03T17:28:14.017457Z"
    }
   },
   "outputs": [],
   "source": [
    "# filters={'location': 'Amsterdam', 'radius': '0', 'min_price': '0', 'max_price': 'ignore_filter'}\n",
    "# apply_basic_filters(driver, filters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-03T20:06:50.791104Z",
     "start_time": "2020-05-03T20:06:23.420418Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Driver initiated successfully!\n",
      "Logged into Google successfully!\n",
      "Aaoooohh, got cought by distil software...or did not solve reCAPTHA in time.\n",
      "Something went wrong :(\n",
      "funda.nl website loaded successfully!\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-6beea0b84de8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Scrape the last x days worth of adverts\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# current_driver, newly_scraped_ads_dict = scrape_new_ads(days=30)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mcurrent_driver\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnewly_scraped_ads_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscrape_new_ads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdays\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl_list_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'url_list_202053'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m56\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_new_ads_pickle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-4-5489c79e73ff>\u001b[0m in \u001b[0;36mscrape_new_ads\u001b[0;34m(filters, days, url_list_name, url_index, use_new_ads_pickle)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;31m# Apply filters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m     \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m     \u001b[0mapply_basic_filters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdriver\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Scrape the last x days worth of adverts\n",
    "# current_driver, newly_scraped_ads_dict = scrape_new_ads(days=30)\n",
    "current_driver, newly_scraped_ads_dict = scrape_new_ads(days=30, url_list_name='url_list_202053', url_index=56, use_new_ads_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-03T14:37:22.746066Z",
     "start_time": "2020-05-03T14:37:22.743586Z"
    }
   },
   "outputs": [],
   "source": [
    "# with open('./Cellar/url_list_202053.pkl', 'rb') as url_list_pickle:\n",
    "#             url_list_pkl = pickle.load(url_list_pickle)\n",
    "\n",
    "# len(url_list_pkl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-03T11:47:49.083094Z",
     "start_time": "2020-05-03T11:47:49.071540Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "306"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('./Cellar/new_adverts.pkl', 'rb') as ads_list_pickle:\n",
    "            newest_ads_list = pickle.load(ads_list_pickle)\n",
    "\n",
    "len(newest_ads_list)\n",
    "# newest_ads_list[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Now that funda is not angry at us anymore\n",
    "# log_in()\n",
    "# filters = {'location': 'Amsterdam', 'radius': '0', 'min_price': '0', 'max_price': 'ignore_filter'}\n",
    "# apply_basic_filters(filters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# updated_data_df = get_recent_ads(days='1', ads_list=current_ads_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# all_data_df = scrape_data(ads_list=current_ads_list, url_list_name='url_list_2020328', url_index=172)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try:\n",
    "#     with open('./Cellar/new_ads.pkl', 'wb') as new_ads_pickle:\n",
    "#              pickle.dump(updated_data_df, new_ads_pickle)\n",
    "# except:\n",
    "#     with open('./Cellar/ads_so_dar_2020329.pkl', 'wb') as ads_so_far_pickle:\n",
    "#              pickle.dump(all_data_df, ads_so_far_pickle)\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
