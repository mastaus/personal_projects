{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-21T14:44:10.205032Z",
     "start_time": "2020-06-21T14:44:10.200743Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:95% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:95% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-21T14:44:10.214445Z",
     "start_time": "2020-06-21T14:44:10.210220Z"
    }
   },
   "outputs": [],
   "source": [
    "#Essentials\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import re\n",
    "import datetime as dt\n",
    "from dateutil.relativedelta import *\n",
    "import time\n",
    "import seaborn as sns\n",
    "\n",
    "#SQL related - NEED TO DECIDE WHICH ONE I'LL BE USING AND DELETE THE REST\n",
    "import sqlite3\n",
    "import pandas.io.sql as pd_sql\n",
    "# import psycopg2\n",
    "# from sqlalchemy import create_engine\n",
    "\n",
    "pd.set_option('display.max_columns', 100)\n",
    "pd.set_option('display.max_rows', 6000)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-21T14:44:10.241151Z",
     "start_time": "2020-06-21T14:44:10.219989Z"
    }
   },
   "outputs": [],
   "source": [
    "def process_and_update(new_ads_df, existing_df, conn):\n",
    "    \"\"\"This function takes the new ads dataframe, the existing data from funda_ads table in the postgres database and the connection string.\n",
    "    It goes through the records updating the is_duplicate, is_price_change and is_status_change columns where needed. \n",
    "    It check for and ignores complete duplicates. Eventually all the records that are not complete duplicates of data in the existing table\n",
    "    are inserted into the database. \n",
    "    Does not return anything.\"\"\"\n",
    "    \n",
    "    # Add a new column that will allow to keep track of which rows to keep and append to the existing table\n",
    "    new_ads_df['is_to_keep'] = False\n",
    "    \n",
    "    # Create lists with only columns used to check for duplicates, status and price changes\n",
    "    check_duplicates = ['property_id', 'title']\n",
    "    check_complete_duplicate = ['property_id', 'title', 'Asking price (€)', 'Status']\n",
    "    check_price_change = ['property_id', 'title', 'Status']\n",
    "    check_status_change = ['property_id', 'title', 'Asking price (€)']\n",
    "\n",
    "    \n",
    "    # The code doesn't yet check if the same records wasn't in the new_ads df letting those records slip through the cracks\n",
    "    \n",
    "    for index, row in new_ads_df.iterrows():        \n",
    "        if row[check_complete_duplicate].values.tolist() in existing_df[check_complete_duplicate].values.tolist():\n",
    "            new_ads_df.at[index, 'is_to_keep'] = False\n",
    "        elif row[check_price_change].values.tolist() in existing_df[check_price_change].values.tolist():\n",
    "            new_ads_df.at[index, 'is_duplicate'] = True\n",
    "            new_ads_df.at[index, 'is_price_change'] = True\n",
    "            new_ads_df.at[index, 'is_to_keep'] = True\n",
    "            # Assign a rank_same_record value\n",
    "            row_id_title_pair = row[check_duplicates].values.tolist()\n",
    "            new_data_df.at[index, 'rank_same_record'] = existing_df[check_duplicates].values.tolist().count(row_id_title_pair) + 1\n",
    "            new_row = new_data_df.iloc[index]\n",
    "            new_row.drop(columns=['listed_date_present', 'is_complete_duplicate', 'is_to_keep'], inplace=True)\n",
    "            new_row.to_sql('funda_ads', con=conn, if_exists='append', index=False)\n",
    "        elif row[check_status_change].values.tolist() in existing_df[check_status_change].values.tolist():\n",
    "            new_ads_df.at[index, 'is_duplicate'] = True\n",
    "            new_ads_df.at[index, 'is_status_change'] = True\n",
    "            new_ads_df.at[index, 'is_to_keep'] = True\n",
    "            row_id_title_pair = row[check_duplicates].values.tolist()\n",
    "            new_data_df.at[index, 'rank_same_record'] = existing_df[check_duplicates].values.tolist().count(row_id_title_pair) + 1\n",
    "            new_row = new_data_df.iloc[index]\n",
    "            new_row.drop(columns=['listed_date_present', 'is_complete_duplicate', 'is_to_keep'], inplace=True)\n",
    "            new_row.to_sql('funda_ads', con=conn, if_exists='append', index=False)\n",
    "        else:\n",
    "            new_ads_df.at[index, 'is_duplicate'] = False\n",
    "            new_ads_df.at[index, 'is_status_change'] = False\n",
    "            new_ads_df.at[index, 'is_to_keep'] = True\n",
    "            row_id_title_pair = row[check_duplicates].values.tolist()\n",
    "            new_data_df.at[index, 'rank_same_record'] = existing_df[check_duplicates].values.tolist().count(row_id_title_pair) + 1\n",
    "            new_row = new_data_df.iloc[index]\n",
    "            new_row.drop(columns=['listed_date_present', 'is_complete_duplicate', 'is_to_keep'], inplace=True)\n",
    "            new_row.to_sql('funda_ads', con=conn, if_exists='append', index=False)\n",
    "            \n",
    "            \n",
    "    final_new_df = new_ads_df[new_ads_df['is_to_keep'] == True] \n",
    "#     final_new_df.drop(columns=['listed_date_present', 'is_complete_duplicate', 'is_to_keep'], inplace=True)\n",
    "    print(f'The database will be updated with additional {final_new_df.count()[0]} records.')\n",
    "    \n",
    "    # Now, update the database table with the new records\n",
    "    final_new_df.to_sql('funda_ads', con=conn, if_exists='append', index=False)\n",
    "    updated_df = pd.read_sql_query(\"\"\"SELECT * FROM funda_ads\"\"\", con=conn)\n",
    "    print(f'Now funda_ads table has {updated_df.count()[0]} records.') \n",
    "    \n",
    "    \n",
    "    # Finally, update the unique_funda_ads table with unique (latest) ads. \n",
    "    # This eliminates all the records where is_price_change and is_status_change is true\n",
    "    updated_df['is_latest_record'] = False\n",
    "    \n",
    "    latest_ads_helper_df = updated_df.groupby('property_id')['rank_same_record'].max().reset_index()\n",
    "    \n",
    "    for index, row in updated_df.iterrows():\n",
    "        if row[['property_id', 'rank_same_record']].values.tolist() in latest_ads_helper_df.values.tolist():\n",
    "            updated_df.at[index, 'is_latest_record'] = True\n",
    "    \n",
    "    unique_records_updated_df = updated_df[updated_df['is_latest_record'] == True] \n",
    "    unique_records_updated_df.to_sql('unique_funda_ads', con=conn, if_exists='replace', index=False)\n",
    "    print(f'unique_funda_ads table has been updated and now has {unique_records_updated_df.count()[0]} records.') \n",
    "\n",
    "\n",
    "def get_listed_date(value, scraped_date):\n",
    "    \"\"\"Converts a string in the 'Listed since' column into a listed_date value (in date format)\"\"\"\n",
    "    today = dt.date.today()\n",
    "\n",
    "    try:\n",
    "        listed_date = dt.datetime.strptime(value, '%B %d, %Y').date()\n",
    "    except:\n",
    "        if scraped_date is np.NaN:\n",
    "            listed_date = np.NaN    \n",
    "        else:         \n",
    "            if 'Today' in value:\n",
    "                listed_date = scraped_date\n",
    "            elif 'week' in value:\n",
    "                weeks_listed = int(re.search('\\d*', value).group())\n",
    "                listed_date = scraped_date - relativedelta(weeks=weeks_listed)\n",
    "            elif 'month' in value:\n",
    "                months_listed = int(re.search('\\d*', value).group())\n",
    "                listed_date = scraped_date - relativedelta(months=months_listed)\n",
    "            elif '6+' in value:\n",
    "                # Not precise enough to tell, could be 7 months, could be 2 years\n",
    "                listed_date = np.NaN\n",
    "            else:\n",
    "                listed_date = np.NaN\n",
    "    return listed_date\n",
    "    \n",
    "\n",
    "def get_energy_label(value):\n",
    "    \"\"\"Takes 'Energy label' column and strips it from the words 'What does this mean?'\n",
    "    Return just the label\"\"\"\n",
    "    no_touch_list = ['Not required', 'Not available', np.NaN]\n",
    "    if value not in no_touch_list:\n",
    "        return value[0]\n",
    "    else: \n",
    "        return np.NaN\n",
    "\n",
    "def get_int(value):\n",
    "    \"\"\"Trims the price, area and other fields with numbers and converts them into int\"\"\"\n",
    "    try:\n",
    "        return re.sub('[€\\sk.,m²m³v.o.n.permonthBeforeAfter]', '', value)\n",
    "    except:\n",
    "        return np.NaN\n",
    "    \n",
    "\n",
    "def get_rooms(value, room_type):\n",
    "    \"\"\"Retrieves the number of rooms specified by the type (room, bedroom, toilet, bathroom, etc.).\n",
    "    If bedrooms are not specified and there is only 1 room - returns 0, if there is more than 1 room, but bedrooms not specified - returns NaN.\n",
    "    Otherwise, returns the number of bedrooms\"\"\"\n",
    "    try:\n",
    "        value = value.lower()\n",
    "    except:\n",
    "        return np.NaN\n",
    "    \n",
    "    if room_type == 'room':\n",
    "        try:\n",
    "            return int(re.search(f'\\d* {room_type}', value).group().strip(f' {room_type}'))\n",
    "        except:\n",
    "            return np.NaN\n",
    "    \n",
    "    if room_type == 'bedroom':\n",
    "        try:\n",
    "            if room_type not in value and int(re.search(f'\\d* {room_type}', value).group().strip(f' {room_type}')) == 1:\n",
    "                return 0\n",
    "            elif room_type not in value:\n",
    "                return np.NaN\n",
    "            else:\n",
    "                return int(re.search(f'\\d* {room_type}', value).group().strip(f' {room_type}'))\n",
    "        except:\n",
    "            return np.NaN\n",
    "    \n",
    "    if room_type == 'toilet':\n",
    "        try:\n",
    "            if room_type not in value and int(re.search(f'\\d* {room_type}', value).group().strip(f' {room_type}')) == 1:\n",
    "                return 1\n",
    "            elif room_type not in value:\n",
    "                return np.NaN\n",
    "            else:\n",
    "                return int(re.search(f'\\d*\\s[a-z]*\\s?{room_type}', value).group().strip(f' separate {room_type}'))\n",
    "        except:\n",
    "            return np.NaN\n",
    "        \n",
    "    if room_type == 'bathroom':\n",
    "        try:\n",
    "            if room_type not in value and int(re.search(f'\\d*\\s[a-z]*\\s?toilet', value).group().strip(f' separate {room_type}')) == 1:\n",
    "                return 1\n",
    "            elif room_type not in value:\n",
    "                return np.NaN\n",
    "            else:\n",
    "                return int(re.search(f'\\d* {room_type}', value).group().strip(f' {room_type}'))\n",
    "        except:\n",
    "            return np.NaN\n",
    "        \n",
    "    \n",
    "def get_bath_flag(value):\n",
    "    \"\"\"Takes Bathroom facilities column and create a Bath_Flag column if a bathtub / bath is available in the property.\n",
    "    Returns True or False\"\"\"\n",
    "    try:\n",
    "        if 'bath' in value.lower():\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    except:\n",
    "        return np.NaN\n",
    "    \n",
    "    \n",
    "def get_facilities(value, facility_type):\n",
    "    \"\"\"Take the Bathroom facilities column and facility type (toilet, shower, bath, jacuzzi, steam cabin, etc.)\n",
    "    and returns the number of specified facilities\"\"\"\n",
    "    try:\n",
    "        value = value.lower()\n",
    "        facility_type = facility_type.lower()\n",
    "    except:\n",
    "        return np.NaN\n",
    "    \n",
    "    try:\n",
    "        return int(re.search(f'\\d* {facility_type}', value).group().strip(f' {facility_type}'))\n",
    "    except:\n",
    "        if facility_type in value:\n",
    "            return 1\n",
    "        else:\n",
    "            return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-21T14:44:39.875179Z",
     "start_time": "2020-06-21T14:44:10.243370Z"
    }
   },
   "outputs": [
    {
     "ename": "OperationalError",
     "evalue": "table funda_ads has no column named 797",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOperationalError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-34-ff554e0a385e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[0;31m# Finally, update the funda_ads postgres table with the new records after removing unnecessary duplicates\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m \u001b[0mprocess_and_update\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_data_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcurrent_data_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-33-4ba3b9dd0e2c>\u001b[0m in \u001b[0;36mprocess_and_update\u001b[0;34m(new_ads_df, existing_df, conn)\u001b[0m\n\u001b[1;32m     30\u001b[0m             \u001b[0mnew_row\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_data_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m             \u001b[0mnew_row\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'listed_date_present'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'is_complete_duplicate'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'is_to_keep'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m             \u001b[0mnew_row\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_sql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'funda_ads'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcon\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mif_exists\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'append'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcheck_status_change\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mexisting_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcheck_status_change\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m             \u001b[0mnew_ads_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mat\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'is_duplicate'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36mto_sql\u001b[0;34m(self, name, con, schema, if_exists, index, index_label, chunksize, dtype)\u001b[0m\n\u001b[1;32m   2128\u001b[0m         sql.to_sql(self, name, con, schema=schema, if_exists=if_exists,\n\u001b[1;32m   2129\u001b[0m                    \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex_label\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mindex_label\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunksize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mchunksize\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2130\u001b[0;31m                    dtype=dtype)\n\u001b[0m\u001b[1;32m   2131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2132\u001b[0m     def to_pickle(self, path, compression='infer',\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/pandas/io/sql.py\u001b[0m in \u001b[0;36mto_sql\u001b[0;34m(frame, name, con, schema, if_exists, index, index_label, chunksize, dtype)\u001b[0m\n\u001b[1;32m    448\u001b[0m     pandas_sql.to_sql(frame, name, if_exists=if_exists, index=index,\n\u001b[1;32m    449\u001b[0m                       \u001b[0mindex_label\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mindex_label\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mschema\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 450\u001b[0;31m                       chunksize=chunksize, dtype=dtype)\n\u001b[0m\u001b[1;32m    451\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    452\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/pandas/io/sql.py\u001b[0m in \u001b[0;36mto_sql\u001b[0;34m(self, frame, name, if_exists, index, index_label, schema, chunksize, dtype)\u001b[0m\n\u001b[1;32m   1479\u001b[0m                             dtype=dtype)\n\u001b[1;32m   1480\u001b[0m         \u001b[0mtable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1481\u001b[0;31m         \u001b[0mtable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minsert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunksize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1482\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1483\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mhas_table\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/pandas/io/sql.py\u001b[0m in \u001b[0;36minsert\u001b[0;34m(self, chunksize)\u001b[0m\n\u001b[1;32m    639\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    640\u001b[0m                 \u001b[0mchunk_iter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart_i\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mend_i\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0marr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata_list\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 641\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_execute_insert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunk_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    642\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    643\u001b[0m     def _query_iterator(self, result, chunksize, columns, coerce_float=True,\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/pandas/io/sql.py\u001b[0m in \u001b[0;36m_execute_insert\u001b[0;34m(self, conn, keys, data_iter)\u001b[0m\n\u001b[1;32m   1268\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_execute_insert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1269\u001b[0m         \u001b[0mdata_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1270\u001b[0;31m         \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecutemany\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minsert_statement\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1271\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1272\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_create_table_setup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOperationalError\u001b[0m: table funda_ads has no column named 797"
     ]
    }
   ],
   "source": [
    "# First things first, connect to the postgresql database\n",
    "conn = sqlite3.connect('./Database/ams_market_watch.db')  # You can create a new database by changing the name within the quotes\n",
    "# cursor = conn.cursor()\n",
    "\n",
    "# Now pull in existing records from the database table called funda_ads\n",
    "current_data_df = pd.read_sql_query(\"\"\"SELECT * FROM funda_ads\"\"\", con=conn)\n",
    "original_ad_count = len(current_data_df)\n",
    "\n",
    "# Also load the data from new_adverts.pkl file \n",
    "with open('./Cellar/Archive/new_adverts_2020414.pkl', 'rb') as new_ads_pkl:\n",
    "    new_data = pickle.load(new_ads_pkl)\n",
    "# with open('./Cellar/new_adverts.pkl', 'rb') as new_ads_pkl:\n",
    "#     new_data = pickle.load(new_ads_pkl)\n",
    "\n",
    "\n",
    "# This is to create a pandas dataframe with the column sorting as in the dictionaries\n",
    "column_list = []\n",
    "\n",
    "for ad in new_data:\n",
    "    for feat_name in list(ad.keys()):\n",
    "        if feat_name not in column_list:\n",
    "            column_list.append(feat_name)      \n",
    "            \n",
    "\n",
    "# Step 3 - Add \"Listed_date_present\", \"listed_date\" and \"is_duplicate\" columns\n",
    "column_list.append('listed_date')\n",
    "column_list.append('listed_date_present')\n",
    "column_list.append('is_duplicate')\n",
    "column_list.append('is_complete_duplicate')\n",
    "column_list.append('rank_same_record')\n",
    "\n",
    "\n",
    "# Initiate the dataframe with the desired columns\n",
    "new_data_df = pd.DataFrame(columns=column_list)\n",
    "\n",
    "\n",
    "# Iterate over the ads_list and replace \"Listed since\" field with a date (using get_listed_date function), where possible \n",
    "for ad in new_data:\n",
    "    ad['listed_date'] = get_listed_date(ad['Listed since'], ad['scraped_date'])\n",
    "    \n",
    "    \n",
    "check_complete_duplicate = ['property_id', 'title', 'Asking price', 'Status']\n",
    "for ad in new_data:\n",
    "    # Create a list with columns cords for checking if a record is a complete duplicate (cdpl)\n",
    "    ad_check_cdpl_value = [ad.get(key) for key in check_complete_duplicate]\n",
    "    \n",
    "    # Check if new data dataframe does not have this record yet, and if so, add it. Otherwise, skip it\n",
    "    if ad_check_cdpl_value not in new_data_df[check_complete_duplicate].values.tolist():\n",
    "        new_data_df = new_data_df.append(ad, ignore_index=True)\n",
    "    \n",
    "    \n",
    "# Convert columns to the right formats, clean up text values and make them numbers, etc.\n",
    "new_data_df.drop(columns=['price'], inplace=True)\n",
    "new_data_df['property_id'] = new_data_df['property_id'].apply(int)\n",
    "new_data_df['listed_date'] = new_data_df.apply(lambda x: get_listed_date(x['Listed since'], x['scraped_date']), axis=1)\n",
    "new_data_df['address'] = new_data_df['title']+', '+new_data_df['address']\n",
    "new_data_df['Asking price'] = new_data_df['Asking price'].apply(get_int)\n",
    "new_data_df['Asking price per m²'] = new_data_df['Asking price per m²'].apply(get_int)\n",
    "new_data_df['VVE (Owners Association) contribution'] = new_data_df['VVE (Owners Association) contribution'].apply(get_int)\n",
    "new_data_df['Year of construction'] = new_data_df['Year of construction'].apply(get_int)\n",
    "new_data_df['Living area'] = new_data_df['Living area'].apply(get_int)\n",
    "new_data_df['Exterior space attached to the building'] = new_data_df['Exterior space attached to the building'].apply(get_int)\n",
    "new_data_df['Volume in cubic meters'] = new_data_df['Volume in cubic meters'].apply(get_int)\n",
    "new_data_df['Rooms'] = new_data_df['Number of rooms'].apply(get_rooms, room_type='room')\n",
    "new_data_df['Bedrooms'] = new_data_df['Number of rooms'].apply(get_rooms, room_type='bedroom')\n",
    "new_data_df['Bathrooms'] = new_data_df['Number of bath rooms'].apply(get_rooms, room_type='bathroom')\n",
    "new_data_df['Toilets'] = new_data_df['Number of bath rooms'].apply(get_rooms, room_type='toilet')\n",
    "new_data_df['Has_Bathtub'] = new_data_df['Bathroom facilities'].apply(get_bath_flag)\n",
    "new_data_df['Baths'] = new_data_df['Bathroom facilities'].apply(get_facilities, facility_type='bath')\n",
    "new_data_df['Number of Toilets'] = new_data_df['Bathroom facilities'].apply(get_facilities, facility_type='toilet')\n",
    "new_data_df['Showers'] = new_data_df['Bathroom facilities'].apply(get_facilities, facility_type='shower')\n",
    "new_data_df['Energy label'] = new_data_df['Energy label'].apply(get_energy_label)\n",
    "new_data_df['Provisional energy label'] = new_data_df['Provisional energy label'].apply(get_energy_label)\n",
    "\n",
    "# Change the names of the columns to make it clearer / more accurate\n",
    "new_data_df.rename(columns={'Asking price': 'Asking price (€)', 'Asking price per m²': 'Asking price per m² (€)', \n",
    "                       'VVE (Owners Association) contribution': 'VVE contribution (monthly) (€)',\n",
    "                      'Living area': 'Living area (m²)', 'Volume in cubic meters': 'Volume (m³)'}, inplace=True)\n",
    "\n",
    "# Finally, update the funda_ads postgres table with the new records after removing unnecessary duplicates\n",
    "process_and_update(new_data_df, current_data_df, conn=conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-21T14:44:39.876539Z",
     "start_time": "2020-06-21T14:44:10.226Z"
    }
   },
   "outputs": [],
   "source": [
    "# Finally, update the funda_ads postgres table with the new records after removing unnecessary duplicates\n",
    "# process_and_update(new_data_df, current_data_df, conn=conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-21T14:44:39.878215Z",
     "start_time": "2020-06-21T14:44:10.232Z"
    }
   },
   "outputs": [],
   "source": [
    "# Now pull in existing records from the database table called funda_ads\n",
    "funda_ads_df = pd.read_sql_query(\"\"\"SELECT * FROM funda_ads\"\"\", con=conn)\n",
    "\n",
    "print(f'Original number of records was {current_data_df.count()[0]}. The final number of records is {funda_ads_df.count()[0]}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-21T14:44:39.879675Z",
     "start_time": "2020-06-21T14:44:10.239Z"
    }
   },
   "outputs": [],
   "source": [
    "unique_funda_ads_df = pd.read_sql_query(\"\"\"SELECT * FROM unique_funda_ads\"\"\", con=conn)\n",
    "unique_funda_ads_df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-21T14:44:39.881290Z",
     "start_time": "2020-06-21T14:44:11.552Z"
    }
   },
   "outputs": [],
   "source": [
    "funda_ads_df[funda_ads_df[['property_id', 'title']].duplicated()].sort_values(by='property_id', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-21T14:44:39.882935Z",
     "start_time": "2020-06-21T14:44:11.556Z"
    }
   },
   "outputs": [],
   "source": [
    "funda_ads_df[funda_ads_df['property_id'] == 87210295]      #87212811"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-21T14:44:39.884433Z",
     "start_time": "2020-06-21T14:44:11.557Z"
    }
   },
   "outputs": [],
   "source": [
    "type(funda_ads_df.iloc[7551])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
